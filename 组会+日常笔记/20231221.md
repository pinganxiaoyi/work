# 20231221

---

## DGL

- [x] 昨天说的 --- 1h
  空白图出现原因

  ```python
  uproot file is  F:\work\test-proton-vertical-full-100GeV_n-simdigi.root
  logit_classes tensor([1, 1, 1, 1, 1], device='cuda:0')
  No selected edges to fit.
  labels is tensor([659]) tensor([0, 0, 1, 1, 1], dtype=torch.int32)
  logit_classes tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')
  No selected edges to fit.
  labels is tensor([666]) tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)
  logit_classes tensor([1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')
  No selected edges to fit.
  labels is tensor([674]) tensor([1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)
  logit_classes tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')
  Graph label is Graph(num_nodes=12, num_edges=32,
        ndata_schemes={'feat': Scheme(shape=(4,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64), 'train_mask': Scheme(shape=(), dtype=torch.bool), 'val_mask': Scheme(shape=(), dtype=torch.bool), 'test_mask': Scheme(shape=(), dtype=torch.bool)}
        edata_schemes={'weight': Scheme(shape=(1,), dtype=torch.float64), 'dx': Scheme(shape=(1,), dtype=torch.float64), 'dy': Scheme(shape=(1,), dtype=torch.float64), 'dz': Scheme(shape=(1,), dtype=torch.float64), 'dE': Scheme(shape=(1,), dtype=torch.float32), 'theta': Scheme(shape=(1,), dtype=torch.float64), 'phi': Scheme(shape=(1,), dtype=torch.float64), 'label': Scheme(shape=(), dtype=torch.int32), 'train_mask': Scheme(shape=(), dtype=torch.bool), 'val_mask': Scheme(shape=(), dtype=torch.bool), 'test_mask': Scheme(shape=(), dtype=torch.bool)}),Number of nodes: 12, Number of edges: 32        
  labels is tensor([679]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)
  delta_xy_list [3.035680659233928] 4
  ```

  

预测+edge label赋值问题：logit_classes tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')
No selected edges to fit.
labels is tensor([666]) tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)

这里全都是1 那么这个事例不是原初入射的粒子 预测也是


- [ ] 扩大文件查找范围

---

## 李沐

- [ ] 两节

  微调 fine tune 将模型从一个数据集到另外一个数据集 大数据集-->小的

  在 PyTorch 中，`nn.init.xavier_uniform_` 是一个权重初始化函数，它根据 Xavier（或 Glorot）均匀初始化的方法来初始化给定的张量。这种初始化方法特别适用于保持输入和输出方差一致，从而帮助优化深度学习网络中的梯度流。

  您提供的代码片段：

  ```python
  nn.init.xavier_uniform_(finetune_net.fc.weight)
  ```

  这行代码的作用是使用 Xavier 均匀初始化方法来初始化 `finetune_net` 神经网络中全连接层（`fc`）的权重。

  ### Xavier 均匀初始化的原理

  Xavier 初始化的核心思想是保持信号在前向传播和反向传播时在所有层中的方差大致相同。这是通过使权重的方差与层的输入和输出数量有关来实现的。对于均匀分布，Xavier 初始化设置权重的值为：

  $$
  W \sim U\left[-\frac{\sqrt{6}}{\sqrt{n_i+n_{i+1}}}, \frac{\sqrt{6}}{\sqrt{n_i+n_{i+1}}}\right]
  $$
  其中 \( n_i \) 和 \( n_{i+1} \) 分别是当前层的输入和输出单位（神经元）的数量。

  ### 应用场景

  - **深度神经网络**：在训练深度神经网络时，合适的权重初始化非常重要，因为不恰当的初始化可能导致梯度消失或爆炸，特别是在深层网络中。

  - **全连接层和卷积层**：尽管 Xavier 初始化最初是为全连接层设计的，但它也经常用于卷积层，尤其是在卷积网络较深时。

  - **激活函数**：Xavier 初始化假设激活函数是线性的。因此，它在使用 Sigmoid 或 Tanh 等近似线性激活函数时效果最好。对于 ReLU 激活函数，通常推荐使用 He 初始化（`nn.init.kaiming_uniform_` 或 `nn.init.kaiming_normal_`）。

  在实践中，使用 Xavier 初始化可以作为提高网络训练稳定性和收敛速度的一种方法。但需要注意的是，最佳的权重初始化方法可能取决于具体的网络架构和任务。

  - [ ] OpenCV 的工具包
---

## python

- [ ] 第二单元

upper函数输出大写字母

namedtuple 专属属性：

​	_fields 显示所有name

​	_make(iterable) 生成一个 A.\_make((a,b,c,d))(a,b,c,d)作为参数输入变成一个A类型

​	_asdict() 返回字典类型的name



